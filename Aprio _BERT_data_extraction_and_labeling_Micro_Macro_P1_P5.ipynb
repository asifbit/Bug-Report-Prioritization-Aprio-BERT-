{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf5zdRAMk0WH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ailgsyrzTgHl",
    "outputId": "e7991228-3eae-45e5-82cb-ff1d4018e54f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh15s3tFOZwG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Assuming the JSON file contains: [1, 2, 3, 4, 5]\n",
    "with open('/content/drive/MyDrive/issues.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OzuUJ98nVD9c",
    "outputId": "35eaabfb-937b-479b-9610-a0123aa3e793"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fc58wKrjv2K"
   },
   "outputs": [],
   "source": [
    "# 'id', 'labels',  'name', 'description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkkDROhUX0qk",
    "outputId": "3802f22d-34b6-4e97-ec67-894aedc373df"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Prepare the CSV file for writing\n",
    "csv_file = '/content/drive/MyDrive/output.csv'\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)  # Specify escapechar and quoting\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writerow(['title', 'body', 'id', 'labels',  'name', 'description'])\n",
    "\n",
    "    # Iterate through each record and write its 'title' and 'body' to the CSV\n",
    "    for record in data:\n",
    "        title = record.get('title', '')  # Get the 'title' field or an empty string if it doesn't exist\n",
    "        body = record.get('body', '')    # Get the 'body' field or an empty string if it doesn't exist\n",
    "        id = record.get('id', '')\n",
    "        labels = record.get('labels', '')\n",
    "        name = record.get('name', '')\n",
    "        description = record.get('description', '')\n",
    "\n",
    "\n",
    "\n",
    "        # Write a row with the extracted data\n",
    "        writer.writerow([title, body, id, labels, name, description])\n",
    "\n",
    "print(f'Data saved to {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "-DtRfI7qEzOj",
    "outputId": "11c45ee3-7092-4094-9944-11a28e3e79cf"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/output.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drTDIdmQX0nn",
    "outputId": "cbcbe03f-6840-48c9-faaf-f4aa6e77ccb1"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22QKMwJaVzgT"
   },
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'column1' and 'column2' are the column names you want to join\n",
    "df['text'] = df['title'].fillna('') + ' ' + df['body'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfVG-in7ViwT",
    "outputId": "1c74aa8a-191f-4c8d-d85b-39c11ec3279a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def categorize_sentiment(sentiment_score):\n",
    "    if sentiment_score >= 0.8:\n",
    "        return 1  # Most negative\n",
    "    elif sentiment_score >= 0.6:\n",
    "        return 2\n",
    "    elif sentiment_score >= 0.4:\n",
    "        return 3\n",
    "    elif sentiment_score >= 0.2:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5  # Least negative\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)['compound']\n",
    "    return categorize_sentiment(sentiment)\n",
    "\n",
    "# Assuming df has a column named 'text'\n",
    "df['sentiment_category'] = df['text'].apply(analyze_sentiment)\n",
    "\n",
    "# Print the DataFrame with the sentiment categories\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "4P2lWf95ZK_s",
    "outputId": "c7317104-e69c-4d9e-dfbb-2f1e405ee339"
   },
   "outputs": [],
   "source": [
    "print(df['sentiment_category'].value_counts())\n",
    "ax=sns.countplot(x='sentiment_category', data=df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center',\n",
    "                xytext = (0, 6),\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# Adding x and y titles\n",
    "# plt.xlabel('is_good_for_kids')\n",
    "plt.ylabel('No. of Reviews')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr_ZrlXsahqa"
   },
   "outputs": [],
   "source": [
    "# Save the updated dataset with labels\n",
    "df.to_csv('labeled_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cxb7d2YOdtes",
    "outputId": "5848a96b-2a02-4772-e5c9-bc0c2f8741e9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def categorize_sentiment(sentiment_score):\n",
    "    if sentiment_score >= 0.8:\n",
    "        return 5  # Most positive\n",
    "    elif sentiment_score >= 0.6:\n",
    "        return 4\n",
    "    elif sentiment_score >= 0.4:\n",
    "        return 3\n",
    "    elif sentiment_score >= 0.2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1  # Most negative\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)['compound']\n",
    "    return categorize_sentiment(sentiment)\n",
    "\n",
    "# Assuming df has a column named 'text'\n",
    "df['sentiment'] = df['text'].apply(analyze_sentiment)\n",
    "\n",
    "# Print the DataFrame with the sentiment labels\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "S_y9e9l7dz4Y",
    "outputId": "eeff9e6f-82a4-45b4-f153-44726b562d6a"
   },
   "outputs": [],
   "source": [
    "print(df['sentiment'].value_counts())\n",
    "ax=sns.countplot(x='sentiment', data=df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center',\n",
    "                xytext = (0, 6),\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# Adding x and y titles\n",
    "# plt.xlabel('is_good_for_kids')\n",
    "plt.ylabel('No. of Reviews')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EbrYPceeHtW"
   },
   "outputs": [],
   "source": [
    "# Save the updated dataset with labels\n",
    "df.to_csv('Final_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "LVKmP9GeSmnS",
    "outputId": "1878c705-6a84-4445-f531-0b071c23faeb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ibFvgrPTdv7"
   },
   "outputs": [],
   "source": [
    "print(df['sentiment_scores'].value_counts())\n",
    "ax=sns.countplot(x='sentiment_scores', data=df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center',\n",
    "                xytext = (0, 6),\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# Adding x and y titles\n",
    "# plt.xlabel('is_good_for_kids')\n",
    "plt.ylabel('No. of Reviews')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idqvRZ8zUrI4"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "# Assuming df has a column named 'text'\n",
    "df['sentiment_scores'] = df['text'].apply(analyze_sentiment)\n",
    "\n",
    "# Print the DataFrame with sentiment scores\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_C5qCM1UJUb"
   },
   "source": [
    "# SentiwordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW8JOvc0V3gh",
    "outputId": "de2d2768-35d2-41fa-90ab-a74703e56dbc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Giksk0aYYsP"
   },
   "outputs": [],
   "source": [
    "df['body'] = df['body'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CL9BK4deTdlO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Download SentiWordNet (if not already downloaded)\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def calculate_sentiment_score(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    total_score = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        synsets = list(swn.senti_synsets(token))\n",
    "        if synsets:\n",
    "            # Assume the first synset is the most relevant\n",
    "            sentiment = synsets[0]\n",
    "            total_score += sentiment.pos_score() - sentiment.neg_score()\n",
    "\n",
    "    return total_score\n",
    "\n",
    "# Assuming df has a column named 'text'\n",
    "df['swn_score'] = df['body'].apply(calculate_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60mTPrW9X35j",
    "outputId": "c60be389-6c24-4a3f-a633-afde1009cef3"
   },
   "outputs": [],
   "source": [
    "# Normalize sentiment scores to a 1-5 scale\n",
    "min_score = df['swn_score'].min()\n",
    "max_score = df['swn_score'].max()\n",
    "\n",
    "df['swn_label'] = df['swn_score'].apply(lambda x: round(1 + 4 * (x - min_score) / (max_score - min_score)))\n",
    "\n",
    "# Print the DataFrame with the sentiment labels\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "oRQTMOYVUgUw",
    "outputId": "8e49738e-ffbd-4f9c-b115-50f1066a4d9a"
   },
   "outputs": [],
   "source": [
    "print(df['swn_label'].value_counts())\n",
    "ax=sns.countplot(x='swn_label', data=df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center',\n",
    "                xytext = (0, 6),\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# Adding x and y titles\n",
    "# plt.xlabel('is_good_for_kids')\n",
    "plt.ylabel('No. of Reviews')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZlmcZTSsQdl",
    "outputId": "6d248e1c-6c8e-4340-e085-9371e4a22b28"
   },
   "outputs": [],
   "source": [
    "# Install XGBoost\n",
    "!pip install xgboost\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPULxgryLkf5",
    "outputId": "67275ace-3b4b-48b6-b072-40d9e62d6ce9"
   },
   "outputs": [],
   "source": [
    "# with preprocessing\n",
    "# X = [' '.join(message) for message in df['msg_lemmatized']]\n",
    "# without preprocessing\n",
    "X = df['text']\n",
    "y = df['sentiment_category']  # Labels\n",
    "y=y-1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create an XGBoost Classifier with various hyperparameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    max_depth=6,           # Maximum depth of each tree\n",
    "    n_estimators=100,      # Number of boosting rounds\n",
    "    learning_rate=0.1,     # Step size for updates\n",
    "    subsample=0.8,         # Fraction of samples used for fitting each tree\n",
    "    colsample_bytree=0.8,  # Fraction of features used for fitting each tree\n",
    "    min_child_weight=1,    # Minimum sum of weights required in a child\n",
    "    reg_alpha=0,           # L1 regularization term\n",
    "    reg_lambda=1,          # L2 regularization term\n",
    "    gamma=0,               # Minimum loss reduction required for a split\n",
    "    random_state=42        # Random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_LFzd4zSVH4",
    "outputId": "83b2a111-5cf3-4ff2-cc49-ce9e8c3241b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # 'weighted' for multi-class\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # 'weighted' for multi-class\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' for multi-class\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogr87lVT1y94"
   },
   "source": [
    "# Macro and Micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGO6m1Ss1vHF",
    "outputId": "4b6f041d-4757-45b2-865b-a7a8fb51c6b9"
   },
   "outputs": [],
   "source": [
    "# Macro metrics\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Micro metrics\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Macro Metrics:')\n",
    "print(f'Precision: {precision_macro}')\n",
    "print(f'Recall: {recall_macro}')\n",
    "print(f'F1-Score: {f1_macro}')\n",
    "\n",
    "print('Micro Metrics:')\n",
    "print(f'Precision: {precision_micro}')\n",
    "print(f'Recall: {recall_micro}')\n",
    "print(f'F1-Score: {f1_micro}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWUNLHyWujFv"
   },
   "source": [
    "# for every class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVqLbVC8RYrB",
    "outputId": "3b5ed607-9cc2-4a5b-94d4-6f54aaaf9524"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming y_test contains the true labels and y_pred contains the predicted labels\n",
    "\n",
    "# Calculate classification report\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "# Calculate accuracy separately\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print precision, recall, f1-score, and accuracy for each class\n",
    "for label, metrics in report.items():\n",
    "    if label != 'accuracy':\n",
    "        class_accuracy = metrics[\"support\"] * metrics[\"recall\"]\n",
    "        print(f'Class: {label}')\n",
    "        print(f'Class Accuracy: {class_accuracy}')\n",
    "        print(f'Precision: {metrics[\"precision\"]}')\n",
    "        print(f'Recall: {metrics[\"recall\"]}')\n",
    "        print(f'F1-score: {metrics[\"f1-score\"]}')\n",
    "        print(f'Support: {metrics[\"support\"]}')\n",
    "\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuyaHhyrVkCD"
   },
   "source": [
    "# P1 to P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjycK3CX2ltu"
   },
   "outputs": [],
   "source": [
    "# Create a new column for priority labels\n",
    "df['Priority_Label'] = ''\n",
    "\n",
    "# Define a dictionary to map Likert scale words to numeric values\n",
    "likert_scale = {\n",
    "    'Critical': 1,\n",
    "    'High': 2,\n",
    "    'Medium': 3,\n",
    "    'Low': 4,\n",
    "    'Negligible': 5\n",
    "}\n",
    "\n",
    "# Function to assign labels based on Likert scale words\n",
    "def assign_priority_label(description):\n",
    "    if isinstance(description, str):\n",
    "        for word, value in likert_scale.items():\n",
    "            if word.lower() in description.lower():\n",
    "                return value  # Return numeric value instead of word\n",
    "    # If none of the Likert scale words are found, assign a default value of 5\n",
    "    return 5\n",
    "\n",
    "# Apply the assign_priority_label function to each bug report description\n",
    "df['Priority_Label'] = df['body'].apply(assign_priority_label)\n",
    "\n",
    "# Save the updated dataset with labels\n",
    "df.to_csv('labeled_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "Bnsd26UA2qba",
    "outputId": "caeeab56-1786-4f7c-f26a-f6bbb3a37180"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Priority_Label'].value_counts())\n",
    "ax=sns.countplot(x='Priority_Label', data=df)\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.0f'),\n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha = 'center', va = 'center',\n",
    "                xytext = (0, 6),\n",
    "                textcoords = 'offset points')\n",
    "\n",
    "# Adding x and y titles\n",
    "# plt.xlabel('is_good_for_kids')\n",
    "plt.ylabel('No. of Reviews')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
