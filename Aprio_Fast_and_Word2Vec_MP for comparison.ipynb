{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwEiOppGq_HI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBWVnORXE3Iu"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ailgsyrzTgHl",
    "outputId": "ed28fc68-53a7-492d-f6dd-93af5cb74f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DtRfI7qEzOj"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/Mobile Prioritazitation/Final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyc7O4Z8mpeY"
   },
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and 'column1' and 'column2' are the column names you want to join\n",
    "df['review'] = df['title'].fillna('') + ' ' + df['body'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTphjr6Y1eal"
   },
   "source": [
    "# Calculating Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoRNynUyUUu7",
    "outputId": "15cabd3b-5768-46c6-999d-45ee11f77231"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqeXJzTAl2i4",
    "outputId": "4f7a0d02-dcbd-4de6-cbb3-d909c7125b7d"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSpqFAsQUXyH",
    "outputId": "d46d83e2-ec1f-45be-e096-dcdb9ebcb5cd"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Tokenize text_list into individual words\n",
    "tokenized_list = [text.split('_') for text in df['review']]\n",
    "\n",
    "# Train Word2Vec model on tokenized_list\n",
    "model = gensim.models.Word2Vec(tokenized_list, min_count=1)#, size = 100\n",
    "\n",
    "# Calculate Word2Vec embeddings for each tokenized text in text_list\n",
    "embeddings = []\n",
    "for tokens in tokenized_list:\n",
    "  # Average the Word2Vec embeddings of all words in a text\n",
    "  text_embedding = np.mean([model.wv[word] for word in tokens if word in model.wv], axis=0)\n",
    "  embeddings.append(text_embedding)\n",
    "\n",
    "# Convert embeddings list to a PyTorch tensor\n",
    "embeddings_tensor = torch.tensor(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvGHEbKuUo49",
    "outputId": "00c04f28-6d4c-4d2a-f27d-d67eee8e420e"
   },
   "outputs": [],
   "source": [
    "embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axsS1auPe7XS"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/content/drive/MyDrive/Mobile Prioritazitation/Word2vec.pickle\", \"wb\") as scores:\n",
    "    pickle.dump(embeddings_tensor, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqZpB8NUfPbs"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Mobile Prioritazitation/Word2vec.pickle\", \"rb\") as scores:\n",
    "   embeddings = pickle.load(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRXhD78zYZC1",
    "outputId": "1901e1a9-b6bd-424a-d807-1bb56b281d07"
   },
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdoOOCPBiLWW",
    "outputId": "99eb0e23-b273-49f2-a958-eec43d032a91"
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsuyyvLtmVj8"
   },
   "source": [
    "# CNN with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWTyFtZ2XubW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D,GlobalMaxPooling1D, MaxPooling1D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_wR7R9kvf14"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, df['sentiment'].values, test_size=0.20, random_state=42)\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "lR_eXTZpEuos",
    "outputId": "d0d247ff-5c99-4f15-d94f-e6c8c48eff02"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming X_train and X_test are lists, convert them to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Reshape the input data for CNN\n",
    "X_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100, 1)))\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dropout(rate=0.2))\n",
    "cnn_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_prob = cnn_model.predict(X_test_cnn)\n",
    "y_pred = (y_pred_prob > 0.5).astype('int')\n",
    "\n",
    "# # Print classification report\n",
    "# target_names = ['0', '1']\n",
    "# print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "92TIXf_h-N4c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Print classification report for ensemble model\n",
    "target_names = ['1 Star', '2 Star', '3 Star','4 Star','5 Star']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "# corr, linewidths=.3, cmap=\"RdBu\", annot=True, fmt=\"\"\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='g', linewidths=.2, cmap='RdBu')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "# Calculate and print accuracy, precision, recall, and f1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(accuracy, precision, recall, f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KdNDIigBY29y"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "eLnioqPRY29z"
   },
   "outputs": [],
   "source": [
    "# Macro metrics\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Micro metrics\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Macro Metrics:')\n",
    "print(f'Precision: {precision_macro}')\n",
    "print(f'Recall: {recall_macro}')\n",
    "print(f'F1-Score: {f1_macro}')\n",
    "\n",
    "print('Micro Metrics:')\n",
    "print(f'Precision: {precision_micro}')\n",
    "print(f'Recall: {recall_micro}')\n",
    "print(f'F1-Score: {f1_micro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AgZXG9YNY29z"
   },
   "outputs": [],
   "source": [
    "# Assuming y_test contains the true labels and y_pred contains the predicted labels\n",
    "\n",
    "# Calculate classification report\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "# Calculate accuracy separately\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print precision, recall, f1-score, and accuracy for each class\n",
    "for label, metrics in report.items():\n",
    "    if label != 'accuracy':\n",
    "        class_accuracy = metrics[\"support\"] * metrics[\"recall\"]\n",
    "        print(f'Class: {label}')\n",
    "        print(f'Class Accuracy: {class_accuracy}')\n",
    "        print(f'Precision: {metrics[\"precision\"]}')\n",
    "        print(f'Recall: {metrics[\"recall\"]}')\n",
    "        print(f'F1-score: {metrics[\"f1-score\"]}')\n",
    "        print(f'Support: {metrics[\"support\"]}')\n",
    "\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNIqthKTpXyL"
   },
   "source": [
    "# CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0imge4vCpWoc",
    "outputId": "e02ee3a9-4702-41b1-b7be-ba39b7423499"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize text_list into individual words\n",
    "tokenized_list = [text.split('_') for text in df['review']]\n",
    "\n",
    "# Train FastText model on tokenized_list\n",
    "model = FastText(tokenized_list, min_count=1)  #, size=100 You can specify the size of the embeddings, here I used size=100 as an example\n",
    "\n",
    "# Calculate FastText embeddings for each tokenized text in text_list\n",
    "embeddings = []\n",
    "for tokens in tokenized_list:\n",
    "    # Average the FastText embeddings of all words in a text\n",
    "    text_embedding = np.mean([model.wv[word] for word in tokens if word in model.wv], axis=0)\n",
    "    embeddings.append(text_embedding)\n",
    "\n",
    "# Convert embeddings list to a NumPy array\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Now you can use embeddings_array as your FastText word embeddings\n",
    "print(embeddings_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_h0_GCQyblN"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/content/drive/MyDrive/Mobile Prioritazitation/FastText.pickle\", \"wb\") as scores:\n",
    "    pickle.dump(embeddings_array, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrzWL0dryblO"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Mobile Prioritazitation/FastText.pickle\", \"rb\") as scores:\n",
    "   embeddings_array = pickle.load(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "009qpqsSpv8_"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_array, df['sentiment'].values, test_size=0.20, random_state=42)\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "id": "a478mRclpv9A",
    "outputId": "73cabffd-2c7b-449a-a4f7-2780cc227813"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Reshape the input data for CNN\n",
    "X_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100, 1)))\n",
    "cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(units=64, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dropout(rate=0.2))\n",
    "cnn_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_prob = cnn_model.predict(X_test_cnn)\n",
    "y_pred = (y_pred_prob > 0.5).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "id": "noGrmQVBp5Zi",
    "outputId": "904e4776-0aa1-4744-d57e-ea442a198310"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Print classification report for ensemble model\n",
    "target_names = ['1 Star', '2 Star', '3 Star','4 Star','5 Star']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "# corr, linewidths=.3, cmap=\"RdBu\", annot=True, fmt=\"\"\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='g', linewidths=.2, cmap='RdBu')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print accuracy, precision, recall, and f1-score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZHxE7vNbUMT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGO6m1Ss1vHF",
    "outputId": "c26d28ed-d324-4cb6-e83e-4b02476bac6b"
   },
   "outputs": [],
   "source": [
    "# Macro metrics\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Micro metrics\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Macro Metrics:')\n",
    "print(f'Precision: {precision_macro}')\n",
    "print(f'Recall: {recall_macro}')\n",
    "print(f'F1-Score: {f1_macro}')\n",
    "\n",
    "print('Micro Metrics:')\n",
    "print(f'Precision: {precision_micro}')\n",
    "print(f'Recall: {recall_micro}')\n",
    "print(f'F1-Score: {f1_micro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVqLbVC8RYrB",
    "outputId": "14600ce7-9f08-4495-a2d3-734279dc23b9"
   },
   "outputs": [],
   "source": [
    "# Assuming y_test contains the true labels and y_pred contains the predicted labels\n",
    "\n",
    "# Calculate classification report\n",
    "target_names = ['1', '2', '3', '4', '5']\n",
    "report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "\n",
    "# Calculate accuracy separately\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print precision, recall, f1-score, and accuracy for each class\n",
    "for label, metrics in report.items():\n",
    "    if label != 'accuracy':\n",
    "        class_accuracy = metrics[\"support\"] * metrics[\"recall\"]\n",
    "        print(f'Class: {label}')\n",
    "        print(f'Class Accuracy: {class_accuracy}')\n",
    "        print(f'Precision: {metrics[\"precision\"]}')\n",
    "        print(f'Recall: {metrics[\"recall\"]}')\n",
    "        print(f'F1-score: {metrics[\"f1-score\"]}')\n",
    "        print(f'Support: {metrics[\"support\"]}')\n",
    "\n",
    "\n",
    "# Print overall accuracy\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
